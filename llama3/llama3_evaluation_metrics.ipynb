{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93014463-791e-4f0c-a06f-e23c1d0f10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from bert_score import score as bertscore\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4169ad1c-d85e-4f5b-b322-848b7d05ba0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_df = pd.read_csv(\"llama3_sample_summaries.csv\")\n",
    "ref_df = pd.read_csv(\"llama3_bhc_matched_reference_summaries.csv\")\n",
    "\n",
    "df = pd.merge(llama_df, ref_df, on=\"note_id\")\n",
    "generated = df['summary'].fillna(\"\").tolist()\n",
    "reference = df['target'].fillna(\"\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2925e6af-8385-463c-9153-07227d0d4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š ROUGE Scores:\n",
      "ROUGE1: 0.2947\n",
      "ROUGE2: 0.1072\n",
      "ROUGEL: 0.1772\n",
      "ROUGELSUM: 0.1828\n"
     ]
    }
   ],
   "source": [
    "### 1. ROUGE\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge_result = rouge.compute(predictions=generated, references=reference)\n",
    "\n",
    "print(\"\\nðŸ“Š ROUGE Scores:\")\n",
    "for k, v in rouge_result.items():\n",
    "    print(f\"{k.upper()}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407909bd-8958-4c44-95c1-a68ae6cd2068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š BLEU Score (average over 1000 rows):\n",
      "BLEU: 0.0312\n"
     ]
    }
   ],
   "source": [
    "### 2. BLEU (average over all rows)\n",
    "smoothie = SmoothingFunction().method4\n",
    "bleu_scores = [\n",
    "    sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
    "    for ref, pred in zip(reference, generated)\n",
    "]\n",
    "bleu_avg = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "print(\"\\nðŸ“Š BLEU Score (average over 1000 rows):\")\n",
    "print(f\"BLEU: {bleu_avg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "889115a0-d6f0-468e-ae14-2202fba2aeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433a08ac9af74920b6985b689d37ce2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b5722f4f8e4b7e887bf70f9489c0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 25.51 seconds, 39.21 sentences/sec\n",
      "\n",
      "ðŸ“Š BERTScore:\n",
      "Precision: 0.8577\n",
      "Recall:    0.8168\n",
      "F1 Score:  0.8365\n"
     ]
    }
   ],
   "source": [
    "### 3. BERTScore\n",
    "P, R, F1 = bertscore(generated, reference, lang=\"en\", verbose=True)\n",
    "bert_avg = {\n",
    "    \"precision\": P.mean().item(),\n",
    "    \"recall\": R.mean().item(),\n",
    "    \"f1\": F1.mean().item()\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š BERTScore:\")\n",
    "print(f\"Precision: {bert_avg['precision']:.4f}\")\n",
    "print(f\"Recall:    {bert_avg['recall']:.4f}\")\n",
    "print(f\"F1 Score:  {bert_avg['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db91b59-13b3-4c3b-aac4-2d02a45f691d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
